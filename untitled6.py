# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c6Lnm0lMahk4FRVyUzV3PYxNdrvr6ByI
"""

# Data handling
import pandas as pd       # For data manipulation using DataFrames
import numpy as np        # For numerical computations

# Visualization
import seaborn as sns     # For advanced statistical plots

# Scikit-learn (ML)
import sklearn as sk                      # Entire sklearn module (generally not used like this)
from sklearn import linear_model         # For linear models like LinearRegression, LogisticRegression
from sklearn import tree                 # For decision tree models
from sklearn import ensemble             # For ensemble methods like RandomForest, GradientBoosting
from sklearn import svm                  # For Support Vector Machines

# XGBoost (Advanced boosting algorithm)
import xgboost                           # For extreme gradient boosting models

# Importing the data
data = pd.read_csv("/traffic volume.csv")

# Displaying the first 5 rows of the data
data.head()

data.describe()

data.info()

from collections import Counter  # Make sure this is imported

# Display null values for each column
print(data.isnull().sum())

# Fill missing numerical values with the column mean
data['temp'].fillna(data['temp'].mean(), inplace=True)
data['rain'].fillna(data['rain'].mean(), inplace=True)
data['snow'].fillna(data['snow'].mean(), inplace=True)

# Check unique weather values and their counts, including missing (NaN)
print(Counter(data['weather']))

# Fill missing weather values with the most frequent one ('Clouds')
data['weather'].fillna('Clouds', inplace=True)

# Only include numeric columns in the correlation calculation
numeric_data = data.select_dtypes(include='number')
correlation = numeric_data.corr()

# Visualize it
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
sns.heatmap(correlation, annot=True, cmap="coolwarm", linewidths=0.5)
plt.title("Correlation Matrix")
plt.show()

data['holiday_encoded'] = data['holiday'].astype('category').cat.codes

sns.heatmap(corr_matrix)



sns.pairplot(data)

data.boxplot()

# Split date column into day, month, year
data[['day', 'month', 'year']] = data['date'].str.split('-', expand=True)

# Split Time column into hours, minutes, seconds
data[['hours', 'minutes', 'seconds']] = data['Time'].str.split(':', expand=True)

# Drop original date and Time columns
data.drop(columns=['date', 'Time'], inplace=True)

# Display first few rows
data.head()

# Separate target and features
y = data['traffic_volume']
x = data.drop(columns=['traffic_volume'], axis=1)

# Convert categorical columns (e.g., 'holiday', 'weather') to dummy/indicator variables
x = pd.get_dummies(x, columns=['holiday', 'weather'], drop_first=True)

# Now scale the numeric data
from sklearn.preprocessing import scale

x_scaled_array = scale(x)

# Convert scaled data back to DataFrame with column names
x_scaled = pd.DataFrame(x_scaled_array, columns=x.columns)

print(x_scaled.head())

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)

from sklearn import linear_model
from sklearn import tree
from sklearn import ensemble
from sklearn import svm
import xgboost

lin_reg = linear_model.LinearRegression()
Dtree = tree.DecisionTreeRegressor()
Rand = ensemble.RandomForestRegressor()
svr = svm.SVR()
XGB = xgboost.XGBRegressor()

lin_reg.fit(x_train, y_train)
Dtree.fit(x_train, y_train)
Rand.fit(x_train, y_train)
svr.fit(x_train, y_train)
XGB.fit(x_train, y_train)



p1 = lin_reg.predict(x_train)
p2 = Dtree.predict(x_train)
p3 = Rand.predict(x_train)
p4 = svr.predict(x_train)
p5 = XGB.predict(x_train)

from sklearn import metrics

print(metrics.r2_score(y_train, p1))
print(metrics.r2_score(y_train, p2))
print(metrics.r2_score(y_train, p3))
print(metrics.r2_score(y_train, p4))
print(metrics.r2_score(y_train, p5))

from sklearn import metrics

# Predict on test set for all models
p1 = lin_reg.predict(x_test)
p2 = Dtree.predict(x_test)
p3 = Rand.predict(x_test)
p4 = svr.predict(x_test)
p5 = XGB.predict(x_test)

# Print R2 scores on test set (true values first, predicted second)
print(metrics.r2_score(y_test, p1))
print(metrics.r2_score(y_test, p2))
print(metrics.r2_score(y_test, p3))
print(metrics.r2_score(y_test, p4))
print(metrics.r2_score(y_test, p5))

from sklearn import metrics
import numpy as np

MSE = metrics.mean_squared_error(y_test, p3)  # true values first, predicted second
RMSE = np.sqrt(MSE)
print(RMSE)

import pickle
pickle.dump(Rand,open("model.pkl",'wb'))

